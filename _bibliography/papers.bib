---
---

@inproceedings{
liao2025llms,
title={{LLM}s as Research Tools: A Large Scale Survey of Researchers{\textquoteright} Usage and Perceptions},
author={Zhehui Liao and Maria Antoniak and Inyoung Cheong and Evie Yu-Yen Cheng and Ai-Heng Lee and Kyle Lo and Joseph Chee Chang and Amy X Zhang},
booktitle={Second Conference on Language Modeling},
year={COLM 2025},
url={https://openreview.net/forum?id=p0BwJk3R1p},
abstract = {The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers actually use LLMs and why or why not. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into aspects of their research workflow. We also find that some traditionally disadvantaged groups in academia (non-white, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns. Our study provides much-needed evidence, rather than speculation, about how LLMs are currently being used as research tools.},
preview={llm.png},
selected={true}
}

@article{10.1145/3757460,
author = {Liao, Zhehui and Zhao, Hanwen and Kulkarni, Ayush and Chattrath, Shaan and Zhang, Amy X.},
title = {Building Proactive and Instant-Reactive Safety Designs to Address Harassment in Social Virtual Reality},
year = {CSCW 2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757460},
doi = {10.1145/3757460},
abstract = {Social Virtual Reality (VR) games offer immersive socialization experiences but pose significant challenges of harassment. Common solutions, such as reporting and moderation, address harassment after it happens but fail to prevent or stop harassment in the moment. In this study, we explore and design proactive and instant-reactive safety designs to mitigate harassment in social VR. Proactive designs prevent harassment from occurring, while instant-reactive designs minimize harm during incidents. We explore three directions for design: user-initiated personal bubbles, clarifying social norms, and encouraging bystander intervention. Through an iterative process, we first conducted a formative interview study to determine design goals for making these features effective, fit user needs, and robust to manipulation. We then implemented Puffer, an integrated safety system that includes a suite of proactive and instant-reactive features, as a social VR prototype. From an evaluation using simulated scenarios with participants, we find evidence that Puffer can help protect players during emergencies, foster prosocial norms, and create more positive social interactions. We conclude by discussing how system safety features can be designed to complement existing proactive and instant-reactive strategies, particularly for people with marginalized identities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
articleno = {CSCW279},
numpages = {38},
keywords = {anti-harassment safety designs, online harassment, social virtual reality},
preview={puffer_preview.jpg},
selected={true}
}